<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head><title></title>
        <meta name="description" content="Liquibase Enterprise Documentation" />
    </head>
    <body>
        <h1>Initializing the Reference Database (REFDB)</h1>
        <MadCap:snippetText src="../../../../Resources/Snippets/images/.ver_icon_size_enterprise.flsnp" />
        <h2 id="InitializingtheReferenceDatabase(REFDB)-Overview">Overview</h2>
        <p>For Oracle, database objects are contained in a schema. Applications will reference one or more schemas.</p>
        <p>For SQL Server, DB2 and EDB Postgres, database objects are contained in a database. Applications will reference one or more databases.</p>
        <p><strong><em>To simplify discussion in this section, we will refer the schemas (in Oracle) and databases (in the other platforms) as Reference Databases.</em></strong>
        </p>
        <h2 id="InitializingtheReferenceDatabase(REFDB)-CreatetheReferenceDatabase">Create the Reference Database</h2>
        <p>Each <MadCap:variable name="General.DaticalDB" /> project will have one or more pipelines - a pipeline represents a release.</p>
        <ul>
            <li>Most commonly, a project will have two pipelines - a current development pipeline and a hotfix pipeline (for production patches).</li>
            <li>Some projects will also contain pipelines to allow for multiple streams of development with different release dates.</li>
        </ul>
        <p><span class="confluence-embedded-file-wrapper"><img class="confluence-embedded-image" loading="lazy" src="https://datical-cs.atlassian.net/wiki/download/attachments/896569686/Screen%20Shot%202018-08-10%20at%205.07.57%20PM.jpg?version=1&amp;modificationDate=1569858652817&amp;cacheVersion=1&amp;api=v2" /></span></p>
        <p>Each pipeline will require its own reference database. The preferred approach is to setup a shared set of environments that can be used to hold the reference databases across all projects. The number of shared environments is equal to the maximum number of pipelines that could exist in a project. For example, if all projects have either 2 or 3 pipelines, then you would need to have 3 new environments available to setup reference databases.</p>
        <p>The reference database is typically a schema-only copy of the production database (DDL and Stored Logic).</p>
        <p>In addition to DDL and Stored Logic, the reference database might also need to contain some data. This should be the minimum amount of data needed by the application team to package and deploy their DML scripts. This is typically limited to application configuration data and look-up tables (i.e., state, country, zipcodes, etc).</p>
        <h2 id="InitializingtheReferenceDatabase(REFDB)-SetUpReferenceDatabaseCredentials">Set Up Reference Database Credentials</h2>
        <p>Once the reference databases are setup, <MadCap:variable name="General.DaticalDB" /> will need database credentials to these new environments. In addition to the typical credentials needed to deploy database changes, <MadCap:variable name="General.DaticalDB" /> needs additional permissions so the packaging process can backup and restore the Reference Database as part of the build process. Documentation on the specific credentials needed can be found in the following database-specific documents:</p>
        <ul>
            <li>Oracle Roles and Permissions for Liquibase Enterprise</li>
            <li><a href="../../../devops-engineer/db-release-pipelines/oracle-amazon-rds-setup/roles-perm-oracle-amazon/roles-oracle-amazon-rds.htm" rel="nofollow">Oracle on Amazon RDS Roles and Permissions</a>
            </li>
            <li><a href="../../../devops-engineer/db-release-pipelines/sql-setup/roles-permissions-sql-server.htm" rel="nofollow">SQL Server Roles and Permissions for Liquibase Enterprise</a>
            </li>
            <li><a href="../../../devops-engineer/db-release-pipelines/azure-sql-setup/roles-azure-sql.htm" rel="nofollow">Azure SQL Database Roles and Permissions</a>
            </li>
            <li><a href="../../../devops-engineer/db-release-pipelines/postgres-edb-postgressql/postgres-edb-PostgreSQL.htm" rel="nofollow">Postgres EDB and PostgreSQL Database Setup</a>
            </li>
            <li><a href="../../../devops-engineer/db-release-pipelines/db2-on-luw-database-setup/roles-perm-db2-luw.htm" rel="nofollow">DB2 LUW Roles and Permissions for Liquibase Enterprise</a>
            </li>
        </ul>
        <h2 id="InitializingtheReferenceDatabase(REFDB)-SetupToUsetheReferenceDatabasetheFirstTime">Setup To Use the Reference Database the First Time</h2>
        <p><span class="confluence-embedded-file-wrapper"><img class="confluence-embedded-image" loading="lazy" src="https://datical-cs.atlassian.net/wiki/download/attachments/896569686/image.png?version=1&amp;modificationDate=1569858653542&amp;cacheVersion=1&amp;api=v2" /></span></p>
        <p>The above image represents a release pipeline in a <MadCap:variable name="General.DaticalDB" /> project. Once this pipeline has been created in a <MadCap:variable name="General.DaticalDB" /> project, the next step is to add a <MadCap:variable name="General.DaticalDB" /> Reference Database.</p>
        <p><span class="confluence-embedded-file-wrapper"><img class="confluence-embedded-image" loading="lazy" src="https://datical-cs.atlassian.net/wiki/download/attachments/896569686/image-1.png?version=1&amp;modificationDate=1569858653345&amp;cacheVersion=1&amp;api=v2" /></span></p>
        <p>Before adding this database as the first step of your pipeline, you will need to create it and populate it as described in the <a href="#" rel="nofollow">Create the Reference Database</a> section above. Once the database has been added to the pipeline, you will need to follow the process below to handle any sql scripts that have not yet been deployed into production.</p>
        <ol>
            <li>Package the sql scripts that have not yet been deployed to production using Datical Packager
				<ol><li>Developer will check-in scripts to be packaged</li><li>Datical Packaging Process will add the changes into the Datical changelog and update the Reference Database</li><li>Build job will create the deploy artifact and load it into the artifact repository</li><li>After all of the sql scripts have been processed, you will be ready to move to the next step</li></ol></li>
            <li>Tell Datical which sql scripts have already been deployed manually in the DEV through STAGE environments<br /><ol><li>For each environment, perform the following Datical command
						<ul><li>Run Datical ChangeLogSync to "fake-deploy" the sql scripts (using the sql script names as labels) that have already been deployed manually into the environment.
								<ul><li>This can be done using the GUI or running the ChangeLogSync job in your deployment automation platform (the job might need to be created if it hasn't already been setup by your Datical PSE as part of the implementation).</li><li>This activity is typically done by the on-boarding engineer to ensure this operation is performed correctly and access is limited to the ChangelogSync job.</li></ul></li><li>Repeat the above step for each environment</li></ul></li><li>After this has been done, everything will be tracked correctly and everyone on the team will be able to package and deploy scripts using Datical from this point forward.</li></ol></li>
        </ol>
    </body>
</html>