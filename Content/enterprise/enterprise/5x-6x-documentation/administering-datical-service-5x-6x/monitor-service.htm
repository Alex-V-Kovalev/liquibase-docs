<?xml version="1.0" encoding="utf-8"?>
<html>
    <head><title></title>
    </head>
    <body>
        <h1>Monitoring Datical Service</h1>
        <p />
        <div class="toc-macro client-side-toc-macro  conf-macro output-block"></div>
        <h1 id="MonitoringDaticalService-HelpwithCLICommands">Help with CLI Commands</h1>
        <div class="code panel pdl conf-macro output-block" style="border-width: 1px;">
            <div class="codeContent panelContent pdl"><pre class="syntaxhighlighter-pre" xml:space="preserve">datical-control --help
Possible commands:
    appdb    (Commands for managing the application database)
    auditdb    (Commands for managing the audit database)
    backup    (Commands for backing up Datical Service)
    cluster    (Commands for managing the Datical Kubernetes cluster)
    dev    (Developer-only commands)
    docker    (Commands for managing Docker)
    etcd    (Commands for managing the Etcd servers)
    gluster    (Commands for the Gluster shared filesystem)
    helm    Executes the `helm` commands
    internal-client    (Commands for managing the internal hammer client (internal Jenkins server))
    jobs    (Commands for managing Datical Service cronjobs)
    kubectl    Executes the `kubectl` command against the datical namespace
    logs [SERVICE|KUBELET]    Outputs service logs
    network    (Commands for managing network settings)
    os    (Commands for managing the underlying OS)
    psql [DATABASE]    Opens an SQL prompt to the PostgreSQL service
    migrate-dmc    Restore DMC 5.3 to Kubernetes 5.4
    rabbitmq    (Commands for interacting with RabbitMQ)
    restore    (Commands for restoring backups)
    secrets    (Commands for managing Kubernetes secrets)
    services    (Commands for managing the Datical Service microservices)
    troubleshoot    Troubleshoots common issues
    upgrade    Upgrades Datical Server and Datical Services as necessary
    versions    (Commands for managing the Datical Service verion used)</pre>
            </div>
        </div>
        <p>
            <br />
        </p>
        <h1 id="MonitoringDaticalService-ProblemIsolation">Problem Isolation</h1>
        <p>Run the following commands to get a top-level view of status.&#160;</p>
        <h2 id="MonitoringDaticalService-Troubleshoot">Troubleshoot&#160;</h2>
        <div class="code panel pdl conf-macro output-block" style="border-width: 1px;">
            <div class="codeContent panelContent pdl"><pre class="syntaxhighlighter-pre" xml:space="preserve">sudo datical-control troubleshoot
=================================================================
====== Checking OS .......PASSED
=================================================================
PASSED: Disk space on /opt/datical/config/cluster
PASSED: Disk space on /opt/datical/data-clustered
PASSED: 1-minute load is 0.81
PASSED: 5-minute load is 2.34

=================================================================
====== Checking Network .......PASSED
=================================================================
PASSED: No packet loss to 10.0.235.233
PASSED: No packet loss to ip-10-0-235-233.ec2.internal
PASSED: Saved hostname ip-10-0-235-233.ec2.internal matches ip-10-0-235-233.ec2.internal
PASSED: 10.0.235.233 is a local hostname
PASSED: ip-10-0-235-233.ec2.internal is a local hostname

=================================================================
====== Checking Cluster Etcd .......PASSED
=================================================================
PASSED: Etcd service is running
PASSED: Etcd listening on 2379
PASSED: Member 10.0.235.233 is in etcd
PASSED: Member 10.0.235.233 health
PASSED: Leader is elected
PASSED: Cluster is healthy

=================================================================
====== Checking Kubernetes nodes .......PASSED
=================================================================
PASSED: All configured hosts are Kubernetes nodes
PASSED: ip-10-0-235-233.ec2.internal is ready
PASSED: At least one node is ready

=================================================================
====== Checking Cluster Pods .......PASSED
=================================================================
PASSED: Kubernetes API listening
PASSED: HTTP ingress on port 80
PASSED: HTTP ingress on port 443
PASSED: Kube-DNS listening
PASSED: Helm tiller listening on 10.106.189.237:44134
PASSED: Pod coredns-5f46cf9549-ngmvm is ready and healthy
PASSED: Pod coredns-5f46cf9549-vgfb5 is ready and healthy
PASSED: Pod filebeat-zdgng is ready and healthy
PASSED: Pod grafana-77f9656d7b-dtkjk is ready and healthy
PASSED: Pod heapster-heapster-557449c69b-kg96m is ready and healthy
PASSED: Pod ingress-nginx-ingress-controller-vshhg is ready and healthy
PASSED: Pod ingress-nginx-ingress-default-backend-856f56c6c6-bw9t4 is ready and healthy
PASSED: Pod ingress-nginx-ingress-default-backend-856f56c6c6-xhgrj is ready and healthy
PASSED: Pod kube-apiserver-ip-10-0-235-233.ec2.internal is ready and healthy
PASSED: Pod kube-controller-manager-ip-10-0-235-233.ec2.internal is ready and healthy
PASSED: Pod kube-proxy-h9zkd is ready and healthy
PASSED: Pod kube-scheduler-ip-10-0-235-233.ec2.internal is ready and healthy
PASSED: Pod kubernetes-dashboard-69c85c54c-q65hg is ready and healthy
PASSED: Pod logstash-7hx6q is ready and healthy
PASSED: Pod prometheus-kube-state-metrics-744f675fd9-ssxwl is ready and healthy
PASSED: Pod prometheus-node-exporter-89k55 is ready and healthy
PASSED: Pod prometheus-pushgateway-ff74f68-5bc59 is ready and healthy
PASSED: Pod prometheus-server-69f88bdd-s2pzp is ready and healthy
PASSED: Pod tiller-deploy-76f886cff-5xnqn is ready and healthy
PASSED: Pod weave-net-v6dkx is ready and healthy

=================================================================
====== Checking GlusterFS .......PASSED
=================================================================
PASSED: Glusterd service is running
PASSED: /opt/datical/config/cluster is mounted
PASSED: /opt/datical/data-clustered is mounted
PASSED: Number of cluster-config bricks is 1
PASSED: Number of cluster-data bricks is 1
PASSED: Write to /opt/datical/config/cluster/troubleshoot.check
PASSED: Saved to brick file /opt/datical/data/gluster/volumes/cluster-config/troubleshoot.check
PASSED: Delete /opt/datical/config/cluster/troubleshoot.check
PASSED: Deleted brick file /opt/datical/data/gluster/volumes/cluster-config/troubleshoot.check
PASSED: Write to /opt/datical/data-clustered/troubleshoot.check
PASSED: Saved to brick file /opt/datical/data/gluster/volumes/cluster-data/troubleshoot.check
PASSED: Delete /opt/datical/data-clustered/troubleshoot.check
PASSED: Deleted brick file /opt/datical/data/gluster/volumes/cluster-data/troubleshoot.check

=================================================================
====== Checking In-Cluster Etcd .......PASSED
=================================================================
PASSED: Member etcd-0 is in etcd
PASSED: Member etcd-0 health
PASSED: Member etcd-1 is in etcd
PASSED: Member etcd-1 health
PASSED: Member etcd-2 is in etcd
PASSED: Member etcd-2 health
PASSED: Leader is elected
PASSED: Cluster is healthy

=================================================================
====== Checking Internal Postgresql .......PASSED
=================================================================
PASSED: Leader sentinel
PASSED: ip_10_0_235_233_ec2_internal keeper is healthy
PASSED: Selected master (Master: ip_10_0_235_233_ec2_internal)
PASSED: Can connect to edge
PASSED: edge databsechangeloglock table is unlocked
PASSED: Can connect to projects
PASSED: projects databsechangeloglock table is unlocked
PASSED: Can connect to reporting
PASSED: reporting databsechangeloglock table is unlocked
PASSED: Can connect to users
PASSED: users databsechangeloglock table is unlocked
PASSED: Can connect to ux
PASSED: ux databsechangeloglock table is unlocked
PASSED: Can connect to auditdb
PASSED: auditdb databsechangeloglock table is unlocked
PASSED: Init job postgresql-init-c14f18f5fe7c00e62d5b8646082842b62e969ff9-29qxf completed

=================================================================
====== Checking RabbitMQ .......PASSED
=================================================================
PASSED: Init job rabbitmq-init-b9353ffb98f0187c412fd2b23cb3be71445c7b8e-w8nck completed
PASSED: Correct number of cluster nodes
PASSED: rabbit@rabbitmq-0.rabbitmq-discovery.datical.svc.cluster.local node_health_check
PASSED: datical.edge.request.unrouted home node
PASSED: datical.edge.request.unrouted mirrors
PASSED: datical.edge.request.unrouted synchronized mirrors
PASSED: datical.projects.requests home node
PASSED: datical.projects.requests mirrors
PASSED: datical.projects.requests synchronized mirrors
PASSED: datical.edge.requests home node
PASSED: datical.edge.requests mirrors
PASSED: datical.edge.requests synchronized mirrors
PASSED: datical.users.requests home node
PASSED: datical.users.requests mirrors
PASSED: datical.users.requests synchronized mirrors
PASSED: datical.reporting.requests home node
PASSED: datical.reporting.requests mirrors
PASSED: datical.reporting.requests synchronized mirrors
PASSED: spring.gen-p179Dp90Rq674-7dXfWwrA home node
PASSED: datical.ux.requests home node
PASSED: datical.ux.requests mirrors
PASSED: datical.ux.requests synchronized mirrors

=================================================================
====== Checking Datical Pods .......PASSED
=================================================================
PASSED: Pod edge-55b8c74b56-qlcb6 is ready and healthy
PASSED: Pod elasticsearch-0 is ready and healthy
PASSED: Pod etcd-0 is ready and healthy
PASSED: Pod etcd-1 is ready and healthy
PASSED: Pod etcd-2 is ready and healthy
PASSED: Pod keycloak-0 is ready and healthy
PASSED: Pod keycloak-1 is ready and healthy
PASSED: Pod keycloak-manager-865dd74d69-d8lws is ready and healthy
PASSED: Pod kibana-59bbd984cb-cn5q8 is ready and healthy
PASSED: Pod postgresql-0 is ready and healthy
PASSED: Pod postgresql-metrics-6fd7b68c78-m7hgz is ready and healthy
PASSED: Pod postgresql-proxy-6f846f77c8-vn5gz is ready and healthy
PASSED: Pod postgresql-proxy-6f846f77c8-wdnvl is ready and healthy
PASSED: Pod postgresql-sentinel-7dbfcc569b-4kzwq is ready and healthy
PASSED: Pod postgresql-sentinel-7dbfcc569b-5pxlh is ready and healthy
PASSED: Pod projects-65b4b598b7-wl26w is ready and healthy
PASSED: Pod rabbitmq-0 is ready and healthy
PASSED: Pod reporting-7797f478b4-vndsg is ready and healthy
PASSED: Pod users-859fbb4c54-mtcxv is ready and healthy
PASSED: Pod ux-6df4fd454f-hzt66 is ready and healthy

=================================================================
====== Checking Datical Jobs .......PASSED
=================================================================
PASSED: Job postgresql-init-c14f18f5fe7c00e62d5b8646082842b62e969ff9-29qxf completed successfully
PASSED: Job rabbitmq-init-b9353ffb98f0187c412fd2b23cb3be71445c7b8e-w8nck completed successfully

=================================================================
====== Checking Keycloak .......PASSED
=================================================================
PASSED: Pod keycloak-0 is Running
PASSED: Pod keycloak-0 health is 1/1
PASSED: Pod keycloak-1 is Running
PASSED: Pod keycloak-1 health is 1/1
PASSED: Keycloak manager is Running
PASSED: Keycloak manager health is 1/1
PASSED: Keycloak manager pod found
PASSED: Keycloak service IP defined
PASSED: Datical realm configured

=================================================================
====== Checking Datical Service .......PASSED
=================================================================
PASSED: Connect to https://ec2-54-165-233-196.compute-1.amazonaws.com
PASSED: /api/v1/cluster/health returns data
PASSED: Cluster is healthy

=================================================================
====== Checking Internal Client .......PASSED
=================================================================
PASSED: Internal client is not enabled. Nothing to check

=================================================================
=================================================================
Datical Troubleshoot Result: PASSED
=================================================================
=================================================================</pre>
            </div>
        </div>
        <h2 id="MonitoringDaticalService-clusterstatus">cluster status</h2>
        <div class="code panel pdl conf-macro output-block" style="border-width: 1px;">
            <div class="codeContent panelContent pdl"><pre class="syntaxhighlighter-pre" xml:space="preserve">sudo datical-control cluster status
Cluster hostname: ec2-54-165-233-196.compute-1.amazonaws.com
Configured hosts:
--------------------------------------
10.0.235.233    |    ip-10-0-235-233.ec2.internal
Kubernetes nodes:
--------------------------------------
NAME                           STATUS    ROLES     AGE       VERSION
ip-10-0-235-233.ec2.internal   Ready     master    13m       v1.11.3
Kubernetes pods:
--------------------------------------
NAME                                                     READY     STATUS    RESTARTS   AGE       IP             NODE                           NOMINATED NODE
coredns-5f46cf9549-ngmvm                                 1/1       Running   0          12m       10.32.0.3      ip-10-0-235-233.ec2.internal   &lt;none&gt;coredns-5f46cf9549-vgfb5                                 1/1       Running   0          12m       10.32.0.2      ip-10-0-235-233.ec2.internal   &lt;none&gt;filebeat-zdgng                                           1/1       Running   0          12m       10.32.0.5      ip-10-0-235-233.ec2.internal   &lt;none&gt;grafana-77f9656d7b-dtkjk                                 1/1       Running   0          11m       10.32.0.14     ip-10-0-235-233.ec2.internal   &lt;none&gt;heapster-heapster-557449c69b-kg96m                       1/1       Running   0          11m       10.32.0.10     ip-10-0-235-233.ec2.internal   &lt;none&gt;ingress-nginx-ingress-controller-vshhg                   1/1       Running   0          11m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;ingress-nginx-ingress-default-backend-856f56c6c6-bw9t4   1/1       Running   0          11m       10.32.0.8      ip-10-0-235-233.ec2.internal   &lt;none&gt;ingress-nginx-ingress-default-backend-856f56c6c6-xhgrj   1/1       Running   0          11m       10.32.0.7      ip-10-0-235-233.ec2.internal   &lt;none&gt;kube-apiserver-ip-10-0-235-233.ec2.internal              1/1       Running   0          12m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;kube-controller-manager-ip-10-0-235-233.ec2.internal     1/1       Running   0          12m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;kube-proxy-h9zkd                                         1/1       Running   0          12m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;kube-scheduler-ip-10-0-235-233.ec2.internal              1/1       Running   0          11m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;kubernetes-dashboard-69c85c54c-q65hg                     1/1       Running   0          11m       10.32.0.6      ip-10-0-235-233.ec2.internal   &lt;none&gt;logstash-7hx6q                                           1/1       Running   0          12m       10.32.0.9      ip-10-0-235-233.ec2.internal   &lt;none&gt;prometheus-kube-state-metrics-744f675fd9-ssxwl           1/1       Running   0          11m       10.32.0.12     ip-10-0-235-233.ec2.internal   &lt;none&gt;prometheus-node-exporter-89k55                           1/1       Running   0          11m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;prometheus-pushgateway-ff74f68-5bc59                     1/1       Running   0          11m       10.32.0.11     ip-10-0-235-233.ec2.internal   &lt;none&gt;prometheus-server-69f88bdd-s2pzp                         2/2       Running   0          11m       10.32.0.13     ip-10-0-235-233.ec2.internal   &lt;none&gt;tiller-deploy-76f886cff-5xnqn                            1/1       Running   0          11m       10.32.0.4      ip-10-0-235-233.ec2.internal   &lt;none&gt;weave-net-v6dkx                                          2/2       Running   0          12m       10.0.235.233   ip-10-0-235-233.ec2.internal   &lt;none&gt;</pre>
            </div>
        </div>
        <h2 id="MonitoringDaticalService-servicesshow">services show</h2>
        <div class="code panel pdl conf-macro output-block" style="border-width: 1px;">
            <div class="codeContent panelContent pdl"><pre class="syntaxhighlighter-pre" xml:space="preserve">datical-control services show
NAME                                   READY     STATUS    RESTARTS   AGE       IP           NODE                           NOMINATED NODE
edge-55b8c74b56-qlcb6                  1/1       Running   0          11m       10.32.0.15   ip-10-0-235-233.ec2.internal   &lt;none&gt;elasticsearch-0                        1/1       Running   0          11m       10.32.0.27   ip-10-0-235-233.ec2.internal   &lt;none&gt;etcd-0                                 1/1       Running   0          11m       10.32.0.31   ip-10-0-235-233.ec2.internal   &lt;none&gt;etcd-1                                 1/1       Running   0          11m       10.32.0.17   ip-10-0-235-233.ec2.internal   &lt;none&gt;etcd-2                                 1/1       Running   0          11m       10.32.0.32   ip-10-0-235-233.ec2.internal   &lt;none&gt;keycloak-0                             1/1       Running   0          11m       10.32.0.19   ip-10-0-235-233.ec2.internal   &lt;none&gt;keycloak-1                             1/1       Running   0          11m       10.32.0.20   ip-10-0-235-233.ec2.internal   &lt;none&gt;keycloak-manager-865dd74d69-d8lws      1/1       Running   0          11m       10.32.0.23   ip-10-0-235-233.ec2.internal   &lt;none&gt;kibana-59bbd984cb-cn5q8                1/1       Running   0          11m       10.32.0.18   ip-10-0-235-233.ec2.internal   &lt;none&gt;postgresql-0                           1/1       Running   0          11m       10.32.0.34   ip-10-0-235-233.ec2.internal   &lt;none&gt;postgresql-metrics-6fd7b68c78-m7hgz    1/1       Running   0          11m       10.32.0.16   ip-10-0-235-233.ec2.internal   &lt;none&gt;postgresql-proxy-6f846f77c8-vn5gz      1/1       Running   0          11m       10.32.0.22   ip-10-0-235-233.ec2.internal   &lt;none&gt;postgresql-proxy-6f846f77c8-wdnvl      1/1       Running   0          11m       10.32.0.21   ip-10-0-235-233.ec2.internal   &lt;none&gt;postgresql-sentinel-7dbfcc569b-4kzwq   1/1       Running   0          11m       10.32.0.24   ip-10-0-235-233.ec2.internal   &lt;none&gt;postgresql-sentinel-7dbfcc569b-5pxlh   1/1       Running   0          11m       10.32.0.26   ip-10-0-235-233.ec2.internal   &lt;none&gt;projects-65b4b598b7-wl26w              1/1       Running   0          11m       10.32.0.25   ip-10-0-235-233.ec2.internal   &lt;none&gt;rabbitmq-0                             1/1       Running   0          11m       10.32.0.35   ip-10-0-235-233.ec2.internal   &lt;none&gt;reporting-7797f478b4-vndsg             1/1       Running   0          11m       10.32.0.28   ip-10-0-235-233.ec2.internal   &lt;none&gt;users-859fbb4c54-mtcxv                 1/1       Running   0          11m       10.32.0.33   ip-10-0-235-233.ec2.internal   &lt;none&gt;ux-6df4fd454f-hzt66                    1/1       Running   0          11m       10.32.0.36   ip-10-0-235-233.ec2.internal   &lt;none&gt;
</pre>
            </div>
        </div>
        <p class="auto-cursor-target">The Status column for each service shows its health.&#160;</p>
        <ul>
            <li class="auto-cursor-target"><strong>Running</strong> - Pod bound to node and all containers created</li>
            <li class="auto-cursor-target"><strong>Pending</strong> - Containers not created. May be waiting for image download.</li>
            <li class="auto-cursor-target"><strong>Succeeded</strong> - All containers terminated successfully and will not be restarted.</li>
            <li class="auto-cursor-target"><strong>Failed</strong> - Container terminated in failure, either exited with non-zero status or was terminated by the system.</li>
            <li class="auto-cursor-target"><strong>Unknown</strong> - Cannot get pod status.</li>
            <li class="auto-cursor-target"><strong>Evicted</strong> - Node is starved for resources and kubelet failed to reclaim sufficient resources to run.&#160;</li>
        </ul>
    </body>
</html>